{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9ac32b",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook generates the intermediate files useful for the interpretability analysis.\n",
    "\n",
    "In particular, for each part of the context it saves the most relevant tokens (2 std above the mean across the entire context) and saves a count.\n",
    "To facilitate aggregate statistics, it saves the average score for each context part, after a normalization between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0cda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from xai import AttributionOutput, AttributionUnit, RowAttribution\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b545e-39b6-45cc-963d-10268c38d400",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In the next cell, there are some variables you can set to generate the intermediate json files for a given explanation target (the part of the output the scores are relevant to) and language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cb02590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"Qwen2.5-72B-Instruct\"\n",
    "LANGUAGE = \"es\"\n",
    "ATTR_TARGET = \"translation_label\" \n",
    "ATTR_TARGET = \"translation\"\n",
    "\n",
    "# TODO: Set the input directory where the raw pickle files containing attribution scores\n",
    "# are located. Set also the id of the set we are studying.\n",
    "INPUT_DIR = \"../../results-interim-gente-xai/attributions/attnlrp/set-g/\"\n",
    "SET = \"set-g\"\n",
    "\n",
    "# Shall we exclude the source tokens from the mean computation? Yes, if we are attributing\n",
    "# the output translation.\n",
    "EXCLUDE_SRC_SCORES = True\n",
    "\n",
    "# Shall we consider the translation label as part of the input context and hence\n",
    "# compute an attribution score to each of its tokens? Yes, if we are attributing\n",
    "# the output translation.\n",
    "ADD_TRANSLATION_LABEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "712cb294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Europarl_ID', 'SET', 'SRC', 'REF-G', 'REF-N', 'COMMON', 'GENDER',\n",
      "       'REF-G_ann', 'G-WORDS', 'translation_label', 'translation',\n",
      "       'raw_output', 'set_label',\n",
      "       'neutrality_label_Qwen/Qwen2.5-72B-Instruct'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "input_file = f\"{INPUT_DIR}/{SET}_{LANGUAGE}_xai_{MODEL}_prompt_v1-4shot.json.attr.pkl\"\n",
    "with open(input_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "if ADD_TRANSLATION_LABEL:\n",
    "    output_file = f\"{INPUT_DIR}/processed-{ATTR_TARGET}_wtl-{MODEL}-{LANGUAGE}.json\"\n",
    "else:\n",
    "    output_file = f\"{INPUT_DIR}/processed-{ATTR_TARGET}-{MODEL}-{LANGUAGE}.json\"\n",
    "\n",
    "translation_file = f\"../../results-interim-gente-xai/attributions/data/{SET}_{LANGUAGE}_xai_{MODEL}_prompt_v1-4shot.json.tsv\"\n",
    "translation_df = pd.read_csv(translation_file, sep=\"\\t\", index_col=\"ID\")\n",
    "print(translation_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dd50cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributionUnit(tokens=['Guid', 'elines', ' for', ' Gender', '-', 'Neutral', ' Translation', ':\\n', '   ', ' -', ' Use', ' neutral', ' synonyms', ' (', 'e', '.g', '.', ' coleg', 'as', ')\\n', '   ', ' -', ' Use', ' neutral', ' collective', ' nouns', ' (', 'e', '.g', '.', ' el', ' profes', 'orado', ',', ' el', ' minister', 'io', ')\\n', '   ', ' -', ' Use', ' neutral', ' re', 'ph', 'ras', 'ings', ' like', ' \"', 'las', ' personas', ' que', '\"', ' or', ' \"', 'qu', 'ien', '/qu', 'ienes', '\"\\n', '   ', ' -', ' Avoid', ' masculine', ' forms', ' for', ' generic', ' refer', 'ents', ' (', 'ending', ' in', ' o', '/os', ')\\n', '   ', ' -', ' Avoid', ' @', ' or', ' x', '\\n', '   ', ' -', ' Avoid', ' double', ' feminine', '/m', 'as', 'cul', 'ine', ' forms', ' (', 'e', '.g', '.', ' amigo', '/a', ')\\n', '    \\n    \\n'], span=(102, 201), metadata=None)\n",
      "##########\n",
      "AttributionUnit(tokens=['You', ' are', ' a', ' helpful', ' Spanish', ' translator', ' specialized', ' in', ' gender', '-neutral', ' language', '.'], span=(3, 15), metadata=None)\n",
      "##########\n",
      "[(AttributionUnit(tokens=['<', 'en', '>', ' I', ' would', ' also', ' like', ' to', ' thank', ' the', ' rapport', 'eur', ',', ' Mrs', ' Ha', 'ug', ',', ' for', ' the', ' magnificent', ' work', ' that', ' she', ' has', ' done', ' as', ' the', ' rapport', 'eur', ' responsible', ' for', ' the', ' budget', ' and', ' for', ' her', ' efforts', ' in', ' dealing', ' with', ' the', ' requests', ' that', ' the', ' various', ' committees', ' have', ' made', '.'], span=(335, 384), metadata={'type': 'gendered'}), AttributionUnit(tokens=['<', 'es', '>', ' **', 'G', 'ENDER', 'ED', '**', ' [', 'Yo', ' también', ' quiero', ' a', 'grad', 'ecer', ' a', ' la', ' pon', 'ente', ',', ' la', ' señ', 'ora', ' Ha', 'ug', ',', ' el', ' magn', 'íf', 'ico', ' trabajo', ' que', ' ha', ' hecho', ' como', ' pon', 'ente', ' de', ' fondo', ' para', ' el', ' presup', 'uesto', ' y', ' su', ' es', 'f', 'uer', 'zo', ' para', ' at', 'ender', ' las', ' p', 'etic', 'iones', ' que', ' hemos', ' hecho', ' desde', ' las', ' distint', 'as', ' com', 'ision', 'es', '.]'], span=(389, 456), metadata={'type': 'gendered'})), (AttributionUnit(tokens=['<', 'en', '>', ' I', ' should', ',', ' moreover', ',', ' like', ' to', ' pay', ' tribute', ',', ' at', ' this', ' point', ',', ' to', ' the', ' deputy', ',', ' Mr', ' Sud', 're', ',', ' for', ' his', ' open', '-minded', 'ness', ' and', ' his', ' consult', 'ative', ' approach', ',', ' which', ' made', ' it', ' possible', ' to', ' successfully', ' complete', ' a', ' report', ' which', ' offers', ' our', ' regions', ' real', ' hope', '.'], span=(201, 253), metadata={'type': 'gendered'}), AttributionUnit(tokens=['<', 'es', '>', ' **', 'G', 'ENDER', 'ED', '**', ' [', 'Qu', 'is', 'iera', ' además', ' salud', 'ar', ' aquí', ' a', ' nuestro', ' dip', 'ut', 'ado', ',', ' el', ' Sr', '.', ' Sud', 're', ',', ' por', ' su', ' esp', 'í', 'rit', 'u', ' de', ' concert', 'ación', ' y', ' de', ' ap', 'ertura', ',', ' que', ' ha', ' permit', 'ido', ' con', ' el', ' es', 'f', 'uer', 'zo', ' de', ' todos', ' llegar', ' a', ' un', ' inform', 'e', ' verd', 'ader', 'amente', ' port', 'ador', ' de', ' esper', 'anza', ' para', ' nuestras', ' region', 'es', '.]'], span=(258, 330), metadata={'type': 'gendered'})), (AttributionUnit(tokens=['<', 'en', '>', ' It', ' is', ' sometimes', ' difficult', ' to', ' get', ' legitimate', ' visitors', ' and', ' groups', ' into', ' this', ' building', ' but', ' those', ' with', ' malign', ' intentions', ' seem', ' to', ' have', ' no', ' difficulty', '.'], span=(546, 573), metadata={'type': 'neutral'}), AttributionUnit(tokens=['<', 'es', '>', ' **', 'NE', 'UT', 'RAL', '**', ' [', 'A', ' veces', ' es', ' difícil', ' conseguir', ' la', ' entrada', ' en', ' este', ' ed', 'ificio', ' de', ' visit', 'antes', ' y', ' grupos', ' leg', 'ít', 'imos', ',', ' pero', ' a', ' los', ' grupos', ' que', ' tra', 'en', ' mal', 'as', ' int', 'enc', 'iones', ' parece', ' result', 'ar', 'les', ' muy', ' fácil', '.]'], span=(578, 626), metadata={'type': 'neutral'})), (AttributionUnit(tokens=['<', 'en', '>', ' We', ' might', ' well', ' hope', ' that', ',', ' before', ' the', ' whole', ' of', ' this', ' package', ' is', ' in', ' place', ',', ' there', ' will', ' be', ' very', ' many', ' more', ' of', ' us', ',', ' as', ' citizens', ',', ' who', ' are', ' affected', ' by', ' these', ' rules', '.'], span=(461, 499), metadata={'type': 'neutral'}), AttributionUnit(tokens=['<', 'es', '>', ' **', 'NE', 'UT', 'RAL', '**', ' [', 'Ant', 'es', ' de', ' que', ' este', ' pa', 'quete', ' de', ' medidas', ' est', 'é', ' list', 'o', ',', ' ser', 'emos', ' muchas', ' más', ' las', ' personas', ' a', ' quienes', ' se', ' nos', ' apl', 'iqu', 'en', '.]'], span=(504, 541), metadata={'type': 'neutral'}))]\n",
      "##########\n",
      "RowAttribution(rid=26, translation_label=AttributionUnit(tokens=[' **', 'NE', 'UT', 'RAL', '**'], span=(3, 8), metadata={'attributions': tensor([ 1.5765e-01, -3.4745e-03, -1.6249e-03, -4.5054e-03, -2.4310e-03,\n",
      "        -2.6964e-03, -2.2551e-03, -1.5992e-02, -7.3440e-03, -2.8673e-03,\n",
      "        -1.4755e-03,  4.6969e-03,  1.0212e-02, -5.9043e-03, -7.6999e-03,\n",
      "        -1.0497e-03, -3.0518e-02, -2.9513e-05,  5.6320e-03, -3.5030e-04,\n",
      "        -6.4378e-03,  1.6878e-04, -7.5364e-04, -5.9261e-03, -2.5480e-03,\n",
      "        -7.6920e-03, -5.0994e-03, -2.9931e-02,  6.7379e-04,  1.3018e-03,\n",
      "         9.5099e-04, -5.0840e-03,  2.3309e-04,  5.7070e-04,  1.6496e-03,\n",
      "         2.4022e-03,  2.7655e-03,  3.6776e-04, -1.1240e-03, -2.4869e-03,\n",
      "         5.3429e-06,  2.4183e-03,  4.4159e-03,  1.2541e-02,  2.3022e-03,\n",
      "         3.5114e-03,  1.1093e-03,  1.6260e-03,  5.6141e-03,  2.1548e-03,\n",
      "        -6.3423e-05,  2.2675e-03,  4.3925e-03,  1.3943e-02,  9.3579e-03,\n",
      "         7.3048e-03,  7.7967e-03,  1.2900e-02,  7.9527e-03,  6.1256e-03,\n",
      "         4.8720e-03,  8.3978e-03,  5.1576e-03,  2.2339e-02,  6.1048e-02,\n",
      "         7.8729e-03,  1.7407e-02,  1.7294e-02,  3.7878e-02,  5.3800e-03,\n",
      "         1.0028e-03,  3.8060e-03,  9.3639e-04,  1.4803e-03, -1.6626e-05,\n",
      "        -3.5069e-04, -2.0328e-03, -6.8608e-04,  2.1125e-03,  3.1778e-03,\n",
      "         2.5349e-03,  5.3098e-03,  1.1274e-03,  9.3971e-04,  2.1197e-04,\n",
      "         1.0338e-03,  7.1583e-03,  2.7164e-03,  5.0190e-03,  4.2865e-03,\n",
      "         3.3668e-02,  3.9275e-03,  4.6148e-03,  5.9545e-03,  1.0441e-03,\n",
      "         1.9187e-02,  5.0152e-02,  1.2114e-01,  5.1716e-02,  5.0938e-02,\n",
      "         7.0579e-03,  3.4338e-03, -7.2666e-04, -2.7388e-03, -3.5409e-04,\n",
      "        -7.1923e-04, -6.6080e-04, -4.8247e-04, -3.9459e-03, -2.8447e-03,\n",
      "        -2.6397e-03, -2.6444e-03, -2.1273e-03,  1.4623e-03, -3.8514e-03,\n",
      "        -1.0477e-03,  1.5751e-04, -1.8299e-03,  2.5608e-04, -4.8635e-03,\n",
      "        -2.8839e-03, -3.3206e-03, -4.7379e-04, -1.5489e-03, -5.8596e-04,\n",
      "        -5.0979e-04, -2.4768e-03, -4.3926e-03, -3.6473e-04, -1.0399e-04,\n",
      "        -9.0225e-04, -1.8256e-04, -1.0290e-03, -3.5169e-04, -2.2206e-03,\n",
      "        -1.4838e-03,  2.2159e-06, -3.3519e-04, -3.7118e-04, -5.8849e-04,\n",
      "        -7.6416e-04, -3.5007e-04, -3.8496e-04,  8.3603e-04, -2.9145e-04,\n",
      "        -1.2124e-03, -1.2105e-03, -8.8179e-04, -1.7499e-03,  1.1268e-03,\n",
      "         2.9471e-03, -1.3908e-03,  2.2196e-03,  3.2890e-03, -3.3531e-04,\n",
      "         2.3642e-03,  5.9795e-03,  2.4508e-02, -1.4109e-03,  4.0650e-03,\n",
      "        -5.7352e-04, -1.2158e-03, -1.4821e-03, -3.5162e-03, -6.4722e-03,\n",
      "        -1.7029e-03, -1.0575e-03, -1.0637e-03, -2.4701e-04, -5.8089e-04,\n",
      "        -1.3288e-03, -1.7713e-03, -1.5729e-03, -1.0696e-03, -2.0963e-03,\n",
      "        -2.6334e-03, -8.4960e-04, -1.1539e-03, -3.7540e-03, -3.1744e-03,\n",
      "        -2.4226e-03, -3.3780e-03, -4.0292e-03, -2.9886e-03, -1.5958e-03,\n",
      "        -7.9859e-05, -2.5398e-04,  1.4876e-03, -1.3471e-03, -1.8506e-03,\n",
      "        -2.7385e-04, -3.8759e-04, -3.7497e-04, -1.3228e-03, -5.7123e-04,\n",
      "        -6.1274e-04, -5.8176e-04, -2.5529e-03, -8.4290e-04, -2.5634e-03,\n",
      "        -5.1177e-03, -1.4412e-02, -2.9526e-02, -1.8293e-02, -1.2515e-02,\n",
      "        -1.7420e-03, -2.8377e-03, -6.2523e-03, -1.6779e-03, -1.6028e-03,\n",
      "        -3.0534e-04,  2.6246e-03, -2.2966e-03, -1.7356e-03, -7.3034e-04,\n",
      "        -8.3210e-04, -8.9877e-04, -1.2200e-03, -1.4008e-03, -1.2500e-03,\n",
      "        -3.5354e-03, -1.4450e-03, -1.8813e-04, -3.5401e-03, -2.6484e-03,\n",
      "        -1.5906e-03, -1.2668e-03, -1.2058e-03, -1.9848e-03, -1.6430e-03,\n",
      "        -9.1596e-04, -1.4354e-03, -6.6366e-04, -1.3058e-03, -9.9270e-04,\n",
      "        -9.8976e-04, -1.3746e-03, -1.0016e-03, -8.7544e-04, -4.7933e-04,\n",
      "        -1.2852e-03, -3.4841e-04, -9.8233e-04, -8.1660e-04, -6.9279e-04,\n",
      "        -1.3970e-03, -1.6666e-03, -1.2078e-03, -7.8395e-04, -2.3141e-03,\n",
      "        -1.7830e-03, -3.8811e-03, -5.5562e-03, -2.3195e-04, -1.2100e-02,\n",
      "         9.4597e-05,  2.9139e-02, -3.6074e-02, -1.2698e-01, -1.4033e-01,\n",
      "        -4.7000e-02,  7.2879e-02, -3.1690e-03,  3.4593e-03,  1.3074e-03,\n",
      "        -8.2212e-03,  1.0673e-03, -1.0604e-04,  1.0286e-04, -7.2722e-03,\n",
      "        -1.7668e-03, -5.0563e-03, -1.1356e-03, -1.4173e-03, -4.6287e-04,\n",
      "        -9.8241e-04, -1.3222e-03, -9.2536e-04, -1.2665e-03, -1.7187e-03,\n",
      "        -1.3073e-03, -2.1052e-03, -7.5219e-04, -6.7497e-04, -8.0616e-04,\n",
      "        -1.0408e-03, -1.3035e-03, -5.8248e-04, -2.7028e-03, -4.3212e-03,\n",
      "        -9.7527e-03, -5.5707e-04, -4.7648e-04, -7.5100e-04, -1.9957e-03,\n",
      "        -6.3791e-04, -6.8067e-04, -9.0232e-04, -1.0823e-03, -9.0148e-04,\n",
      "        -6.9285e-04, -4.0258e-04, -1.0173e-03, -9.5402e-04,  8.7791e-05,\n",
      "        -3.8902e-04, -6.8946e-04, -1.0430e-03, -1.4240e-03, -5.4465e-04,\n",
      "        -1.9654e-04, -4.5244e-04, -2.0717e-03, -3.4813e-04, -2.9852e-04,\n",
      "        -5.7648e-04, -4.3027e-04, -5.5126e-04, -2.9383e-04, -8.6555e-04,\n",
      "        -5.9036e-04, -7.4521e-04, -3.1544e-04, -6.1510e-04, -1.5932e-03,\n",
      "        -6.9495e-04, -2.7469e-04, -9.2888e-04, -7.3120e-04, -7.0195e-03,\n",
      "        -5.0752e-05, -5.2029e-03,  2.1140e-05, -2.5795e-03, -7.1114e-03,\n",
      "        -6.4597e-03, -1.2420e-02, -6.0925e-03, -5.0358e-03, -2.0559e-03,\n",
      "        -3.6440e-03, -2.0242e-03, -6.9466e-04, -2.9748e-03, -9.1282e-04,\n",
      "        -4.4992e-03, -2.8724e-03, -2.3417e-03, -6.8567e-03, -2.3195e-03,\n",
      "        -1.7001e-03, -2.3515e-03, -9.5902e-04, -6.9028e-04, -1.6352e-03,\n",
      "        -1.4715e-03, -9.9227e-04, -1.4408e-03, -8.4145e-04, -6.5576e-04,\n",
      "        -5.5366e-04, -3.6770e-04, -1.5251e-03, -9.0201e-04, -2.5501e-03,\n",
      "        -8.4392e-04, -9.1799e-04, -1.7012e-03, -1.0968e-03, -5.7152e-04,\n",
      "        -1.0006e-03, -1.3262e-03, -1.5349e-03, -1.5077e-03, -9.6041e-04,\n",
      "        -7.2786e-04, -2.4395e-03, -9.3771e-04, -6.9160e-04, -1.5024e-03,\n",
      "        -1.3974e-03, -8.4670e-04, -8.9823e-04, -6.3949e-03, -1.7872e-04,\n",
      "        -1.0842e-02,  1.0140e-05, -2.3839e-02, -2.6068e-02, -6.9577e-02,\n",
      "        -6.8033e-02, -2.1664e-02, -7.8280e-03,  3.3554e-03,  8.1475e-03,\n",
      "         2.9107e-03,  7.4953e-05, -1.0838e-02, -1.0851e-02, -2.4061e-03,\n",
      "        -2.2207e-03, -1.4164e-03, -2.9730e-03, -5.1020e-04, -3.0885e-04,\n",
      "        -1.7256e-03, -1.0498e-03, -9.6569e-04, -9.6254e-04, -1.2227e-03,\n",
      "        -9.7002e-04, -6.8136e-04, -3.9452e-04,  2.1758e-04, -2.6033e-04,\n",
      "        -1.6947e-04, -2.7600e-04, -4.6338e-04, -1.7483e-04, -5.8344e-04,\n",
      "        -8.4069e-04, -1.4888e-04, -2.7546e-04, -7.6313e-04, -1.8048e-04,\n",
      "         5.5047e-05,  4.8705e-04,  8.0547e-04, -2.6572e-05, -2.9003e-04,\n",
      "        -8.3033e-04, -7.2264e-04, -4.0504e-04, -1.0594e-04, -5.7651e-04,\n",
      "        -8.3087e-04, -2.4878e-03, -3.9508e-04, -3.8492e-04, -6.4443e-04,\n",
      "        -4.1144e-04, -2.2790e-04, -9.7833e-05, -2.7818e-04, -9.3840e-04,\n",
      "        -3.3835e-04, -1.3115e-04, -2.8889e-04, -5.6369e-04, -9.2858e-04,\n",
      "        -5.0023e-04, -9.8071e-05, -7.0217e-04, -5.9803e-04, -7.2516e-04,\n",
      "        -1.9295e-03,  7.1326e-05, -5.7740e-03, -8.1279e-06, -8.1782e-04,\n",
      "        -2.3396e-03, -3.8703e-04, -3.2227e-03, -5.0072e-03, -7.6024e-03,\n",
      "        -6.1952e-03, -3.3385e-03, -7.5776e-03, -2.8101e-03, -2.0161e-03,\n",
      "        -4.4884e-03, -9.2120e-04, -1.9236e-03, -1.0442e-03, -8.4846e-04,\n",
      "        -3.5880e-03, -7.5339e-04, -6.6924e-04, -9.8990e-04, -1.8817e-03,\n",
      "        -2.3075e-03, -1.1442e-03, -4.8749e-04, -1.5343e-03, -1.9522e-03,\n",
      "        -9.0066e-04, -5.7947e-04, -1.7880e-03, -1.4876e-03, -1.0795e-03,\n",
      "        -4.5312e-03, -1.0721e-03, -1.9265e-03, -1.2894e-03, -3.1515e-03,\n",
      "        -8.3614e-04, -1.1037e-03, -3.2979e-03, -5.9367e-03, -1.4017e-04,\n",
      "        -7.9913e-03,  8.6831e-06, -1.2059e-02, -1.8964e-02, -4.8735e-02,\n",
      "        -4.8701e-02, -1.7103e-02,  3.1515e-02,  1.0442e-02,  4.5595e-03,\n",
      "         7.0491e-03,  9.1211e-03, -1.1046e-02, -1.8646e-03, -1.3068e-03,\n",
      "        -4.0559e-04, -4.9975e-04, -2.3206e-04, -1.4958e-03, -1.0057e-03,\n",
      "        -3.2806e-04, -2.0142e-03, -4.6136e-04, -4.6213e-04, -1.8179e-03,\n",
      "        -5.9491e-04, -1.8393e-03,  1.5927e-04,  8.3654e-04, -3.3056e-03,\n",
      "        -2.5922e-04,  2.3733e-03,  3.2720e-04, -3.4528e-04, -1.1346e-03,\n",
      "         1.3797e-04, -2.7538e-04, -3.3416e-04, -3.9857e-04, -1.7234e-04,\n",
      "         1.4461e-04,  1.3831e-04, -3.0304e-03,  1.2888e-05, -1.9733e-03,\n",
      "        -4.7240e-03, -2.2652e-03, -5.6362e-03, -5.7980e-03, -1.0055e-02,\n",
      "        -4.1056e-03, -8.1059e-03, -4.8072e-03, -4.6459e-04, -1.2658e-03,\n",
      "        -6.6775e-03, -5.5749e-03, -1.9625e-03, -3.6992e-03, -1.9963e-03,\n",
      "        -1.0966e-03, -4.0558e-03, -3.3696e-03, -1.8786e-03, -1.4990e-03,\n",
      "        -6.0279e-03, -2.8761e-03, -2.8441e-03, -2.6841e-04, -1.2999e-03,\n",
      "        -1.2200e-03, -3.0421e-03, -5.0026e-03, -5.3584e-05, -4.5981e-03,\n",
      "        -1.1588e-05, -1.1916e-02, -9.9868e-03, -3.6991e-02, -5.1934e-02,\n",
      "        -2.3740e-02,  2.9714e-02,  4.6965e-03,  3.6466e-03,  7.7125e-03,\n",
      "         3.8826e-03, -8.8768e-03, -1.5885e-03, -4.9438e-03, -2.1597e-03,\n",
      "        -1.7449e-03, -8.2612e-04, -1.5603e-03, -2.0868e-03,  1.8934e-06,\n",
      "        -8.9206e-04, -1.8020e-03, -9.5795e-04, -2.8169e-04,  7.5784e-04,\n",
      "        -1.0731e-03, -8.6942e-04, -1.5311e-03, -1.5073e-03, -2.0939e-03,\n",
      "        -2.1305e-03, -1.8159e-03, -9.1804e-04, -1.9714e-04, -8.0126e-04,\n",
      "        -5.7898e-04, -7.1777e-04, -1.3557e-03, -5.6040e-04, -6.9758e-04,\n",
      "        -1.8386e-04,  2.6516e-04, -8.0227e-06, -1.5914e-03, -2.7296e-03,\n",
      "        -4.9958e-03, -4.8240e-04, -6.1843e-04, -1.1324e-03, -1.5410e-03,\n",
      "        -5.7310e-03,  2.1100e-04, -2.8523e-03,  3.5823e-06, -4.4788e-03,\n",
      "        -8.3897e-03, -4.8902e-03, -9.9450e-03, -1.4994e-02, -4.9731e-02,\n",
      "        -4.6025e-02, -2.5875e-02, -7.3523e-02, -5.4594e-02, -1.6916e-02,\n",
      "        -1.0739e-02,  2.7008e-02, -8.1648e-03, -1.6180e-02, -3.0310e-03,\n",
      "         3.5961e-04, -4.1294e-03, -2.3683e-02, -1.2112e-03, -5.7720e-03,\n",
      "        -1.2321e-04,  6.2165e-02, -6.1381e-02])}), translation=AttributionUnit(tokens=[' [', 'T', 'amb', 'ién', ' quis', 'iera', ' advert', 'ir', ' sobre', ' el', ' ries', 'go', ' de', ' ot', 'org', 'ar', ' perm', 'isos', ' de', ' trabajo', ' a', ' personas', ' migr', 'antes', ' para', ' el', ' trabajo', ' dom', 'ést', 'ico', '.]'], span=(8, 39), metadata={'attributions': tensor([-1.9608e-01, -8.9478e-03, -2.4694e-03, -1.0114e-02, -6.2739e-03,\n",
      "         6.5443e-05, -8.4277e-03,  4.2123e-02, -1.2998e-02, -1.3957e-03,\n",
      "        -2.3260e-03, -4.1469e-03, -2.4982e-02, -1.3450e-04, -2.2527e-02,\n",
      "        -2.8192e-03, -1.9508e-02, -2.7674e-04, -2.6065e-02, -1.3237e-02,\n",
      "        -1.3145e-02, -5.8358e-04, -4.4007e-03, -9.9618e-03, -2.9552e-03,\n",
      "        -3.9257e-04, -3.8651e-03,  1.9451e-02, -2.4317e-03, -3.4658e-03,\n",
      "        -1.0649e-02, -4.0269e-02, -1.4665e-02, -3.0820e-03, -3.0223e-03,\n",
      "        -3.1370e-03, -3.3057e-03,  1.1165e-04, -1.3297e-04, -5.4407e-03,\n",
      "        -2.6930e-03, -2.3062e-03, -2.1121e-03, -4.5335e-03, -3.0033e-03,\n",
      "        -3.7639e-04, -4.1628e-04,  3.1614e-04, -3.7258e-03,  8.5095e-04,\n",
      "        -2.5320e-03, -4.5185e-05,  2.3162e-05,  6.8480e-03, -7.1423e-03,\n",
      "        -4.4766e-03, -2.5982e-03, -6.2633e-03, -6.2245e-03, -3.4506e-03,\n",
      "        -9.0457e-04, -8.4134e-03, -4.4942e-03, -2.4441e-02, -3.8161e-02,\n",
      "         1.5835e-03, -3.4520e-03, -1.1740e-02, -3.4574e-02, -2.0389e-03,\n",
      "        -4.6805e-04, -5.2815e-03, -8.2491e-04, -3.7485e-03,  4.6815e-04,\n",
      "        -9.4503e-05, -3.7286e-03, -2.1441e-03, -1.3900e-03, -2.9104e-03,\n",
      "         5.6719e-04,  2.5250e-03, -5.7874e-04, -2.0322e-03, -3.4383e-04,\n",
      "        -1.2521e-03, -4.0548e-03, -6.7964e-04, -4.8168e-03, -5.4372e-03,\n",
      "        -1.9523e-02, -1.1200e-02, -2.7160e-03, -5.3712e-03, -3.4163e-03,\n",
      "        -2.0120e-02, -2.0978e-02, -5.6096e-02, -3.5101e-02, -2.2804e-02,\n",
      "        -1.2476e-02, -2.5501e-02, -4.3037e-03, -1.4817e-03, -1.0563e-03,\n",
      "         7.6090e-04,  9.7762e-04, -8.2240e-03, -2.5568e-05,  2.2043e-03,\n",
      "         8.6842e-04,  1.4289e-03,  1.7168e-03,  1.0261e-03,  6.4992e-03,\n",
      "         3.2904e-03, -1.2683e-03,  4.7546e-03,  8.0185e-04,  2.3176e-02,\n",
      "         3.3912e-03,  4.7699e-03,  1.8254e-03,  4.9554e-03, -1.4418e-04,\n",
      "         2.5930e-03,  7.2185e-03,  1.3777e-02,  9.4860e-04, -8.7483e-04,\n",
      "         1.5650e-03,  7.2741e-05,  1.4073e-02,  6.2875e-03,  9.1250e-03,\n",
      "         5.3578e-03,  2.9797e-03,  2.6046e-03,  3.0098e-03,  2.1375e-03,\n",
      "         8.4289e-04,  9.3168e-04,  8.0515e-04, -3.0780e-04, -5.4189e-04,\n",
      "        -1.7391e-03, -1.3074e-04,  2.1874e-03,  3.4854e-03,  7.8869e-03,\n",
      "        -1.5862e-02,  1.0302e-02,  1.6776e-02,  1.6889e-02,  8.2064e-03,\n",
      "         1.3323e-02,  2.7634e-02,  1.0929e-01,  1.0052e-02,  2.4316e-02,\n",
      "         4.9536e-03,  4.5623e-05,  6.8690e-04, -1.0868e-03, -3.1412e-04,\n",
      "        -3.1085e-04,  3.3419e-04, -9.8292e-05, -8.1161e-04,  4.2612e-04,\n",
      "        -3.8485e-04,  1.3898e-03, -2.0592e-04, -9.8892e-04, -2.1400e-03,\n",
      "        -9.7966e-04, -2.7451e-04, -8.4641e-04,  3.7745e-03, -2.2650e-02,\n",
      "         2.8502e-03, -8.4480e-04, -1.3558e-03, -1.6940e-03, -1.9800e-05,\n",
      "         1.4677e-03, -1.6684e-03,  8.9265e-05,  1.4407e-03,  4.0958e-03,\n",
      "         3.2870e-05, -5.0552e-04,  4.8380e-04,  4.3067e-04, -4.6863e-04,\n",
      "        -4.1878e-04, -5.2899e-04,  1.0890e-03, -1.8787e-03, -1.0474e-03,\n",
      "         7.5169e-04, -9.5039e-03, -1.4030e-02, -4.6434e-03,  1.0168e-04,\n",
      "         8.5114e-04,  4.5748e-03,  1.6880e-02,  1.1601e-03,  5.4225e-03,\n",
      "         4.5105e-03,  8.0973e-03,  1.0084e-02,  3.4081e-03,  8.8636e-04,\n",
      "         2.4622e-03,  3.5499e-03,  2.0006e-03, -1.4989e-04,  2.1021e-03,\n",
      "         1.2041e-02,  2.8067e-03, -1.1186e-03,  3.2646e-03,  2.1661e-03,\n",
      "         4.6438e-04,  6.7297e-04,  3.2855e-03,  9.8349e-03,  1.2956e-02,\n",
      "         2.0125e-03,  4.2074e-03,  1.4317e-04,  5.8693e-03,  4.4406e-03,\n",
      "         1.7567e-03,  1.7876e-03,  2.2166e-03,  1.1446e-04, -2.5739e-04,\n",
      "         1.7693e-03,  1.2522e-03,  1.7988e-03, -2.5289e-04,  1.5369e-03,\n",
      "         3.6043e-03,  2.8868e-03,  4.8921e-04,  2.9049e-04,  2.1275e-03,\n",
      "         3.6027e-03,  4.0558e-03, -5.5637e-03, -4.4727e-04, -2.4328e-02,\n",
      "         4.3842e-05, -7.6581e-02, -5.4250e-02, -1.2871e-01, -1.2254e-01,\n",
      "        -5.3916e-02, -1.0256e-01,  5.2520e-03, -3.9574e-03, -7.9733e-03,\n",
      "        -6.1527e-03, -1.7609e-02,  1.0186e-02,  1.7079e-02,  1.6269e-02,\n",
      "         1.2221e-02,  1.5188e-02,  5.4587e-03,  1.1050e-02,  4.4300e-03,\n",
      "         4.8877e-03, -7.1539e-03, -4.3111e-03,  6.2106e-03,  7.2597e-03,\n",
      "         2.4335e-03,  7.4749e-03,  2.3059e-03, -1.3429e-03,  2.0698e-03,\n",
      "         2.4313e-03,  4.8118e-03,  2.1127e-04,  6.7067e-03,  1.0992e-02,\n",
      "         1.7194e-02,  4.9521e-03,  5.2927e-03,  1.0389e-02,  6.6363e-03,\n",
      "         2.3899e-03,  3.0591e-03,  1.7839e-03,  2.2747e-03,  3.1185e-03,\n",
      "         3.0510e-03,  7.9586e-04,  3.6036e-03,  5.1915e-03,  2.8936e-03,\n",
      "         8.9221e-04, -3.2091e-04,  1.3650e-03,  2.4121e-03,  3.4255e-03,\n",
      "         1.6069e-03,  2.6260e-03,  1.4838e-02,  2.8615e-03,  9.6339e-04,\n",
      "         4.8814e-03,  4.5685e-03,  3.7290e-03,  8.6643e-04,  3.0043e-03,\n",
      "         4.3301e-03,  5.3825e-03,  9.0810e-04,  3.7232e-03,  6.9234e-03,\n",
      "         5.1238e-03,  1.2045e-03,  3.3118e-03,  2.7447e-03,  7.0292e-03,\n",
      "        -2.7868e-04, -8.2356e-03, -8.2762e-05, -2.0959e-02, -7.9542e-03,\n",
      "        -8.8979e-04,  1.0081e-04,  1.7795e-03,  3.4486e-03,  2.7808e-03,\n",
      "         1.6771e-02,  5.7580e-03, -2.0796e-04,  9.5605e-03,  1.6314e-03,\n",
      "         1.1802e-02,  8.5040e-03,  4.6876e-03,  5.1018e-03,  2.1419e-03,\n",
      "         1.2428e-03,  1.8502e-03,  1.9713e-03,  2.2600e-03,  7.3317e-03,\n",
      "         1.3720e-03,  9.6945e-04,  2.4085e-04,  2.3195e-03,  1.6926e-03,\n",
      "         2.1760e-03,  1.3599e-03,  2.6550e-03,  3.0121e-03,  5.9822e-03,\n",
      "         1.3358e-03,  1.5733e-03,  8.3094e-03,  3.4191e-03, -3.1387e-04,\n",
      "         3.7683e-05,  4.6766e-03, -1.5295e-04,  4.3179e-04,  6.9855e-04,\n",
      "         6.6398e-04,  4.3479e-03,  8.6812e-04,  1.8341e-03,  1.5161e-03,\n",
      "         3.9293e-03,  4.4192e-04, -2.3041e-03,  4.1387e-04, -6.7890e-05,\n",
      "        -4.0928e-03, -6.9237e-05, -2.7513e-02, -2.0123e-02, -4.9848e-02,\n",
      "        -5.0991e-02, -2.0679e-02,  8.7417e-03,  6.9705e-03,  6.6728e-03,\n",
      "        -2.5299e-03,  2.2983e-03, -1.5609e-02,  1.2840e-02,  1.2031e-02,\n",
      "         1.9374e-02,  1.0812e-03, -1.8274e-03,  7.2115e-03,  8.4662e-04,\n",
      "         1.7376e-03,  6.5069e-03,  7.3251e-03,  5.2085e-03,  1.7455e-03,\n",
      "         2.2803e-04,  2.7293e-03,  3.3528e-03,  3.6190e-03,  3.1932e-04,\n",
      "         2.5583e-03,  4.2735e-03,  3.0841e-03,  1.5961e-03,  5.5390e-03,\n",
      "        -6.3081e-04,  1.7395e-03,  5.2952e-03,  1.2972e-02,  1.9088e-03,\n",
      "         1.3486e-03, -1.8261e-03, -1.7239e-02,  4.1318e-03, -8.7308e-06,\n",
      "         4.3397e-03,  7.8027e-03,  1.1824e-03, -8.9890e-04,  1.7950e-03,\n",
      "         2.7295e-03,  9.0829e-03,  3.4727e-03,  4.3422e-03,  1.4059e-03,\n",
      "         4.1223e-03,  5.3669e-03,  2.3201e-03,  3.0614e-03,  9.4585e-03,\n",
      "        -4.8284e-05,  3.7558e-03,  7.1204e-03,  1.6696e-02,  4.5522e-03,\n",
      "         2.9730e-03,  2.7078e-03,  2.3429e-03,  2.0257e-03, -1.4455e-04,\n",
      "         1.5133e-02,  1.6972e-04, -9.5080e-04, -9.9271e-05, -4.2175e-03,\n",
      "         4.1106e-04, -6.7107e-04,  2.9965e-03,  3.4022e-03,  3.1514e-03,\n",
      "         6.0251e-05,  6.8798e-03,  1.3867e-02,  8.0780e-03,  4.4483e-03,\n",
      "         9.5541e-03,  1.7691e-03,  3.6546e-03,  1.5912e-03,  1.7845e-03,\n",
      "         1.7079e-02,  4.6896e-03,  5.1412e-03,  1.1726e-02,  8.0403e-03,\n",
      "         4.3387e-04,  3.3731e-03,  2.8555e-03,  2.4220e-03,  3.1757e-03,\n",
      "         2.1866e-03,  1.7311e-03,  4.2077e-03,  2.5263e-03, -1.6055e-03,\n",
      "         1.1943e-02,  1.3423e-03,  5.9054e-03,  2.9672e-03,  1.2905e-02,\n",
      "         5.0574e-03,  6.4988e-03,  1.6177e-02,  8.8757e-03, -2.3478e-05,\n",
      "        -1.2877e-03, -1.0992e-04, -6.8387e-03, -7.5746e-03, -5.1574e-02,\n",
      "        -3.5340e-02, -1.7593e-02, -2.7784e-02, -1.1116e-02, -4.6101e-03,\n",
      "        -6.4917e-03, -1.3619e-02, -2.6566e-02,  1.7888e-02,  7.2084e-03,\n",
      "         4.0063e-03,  6.8977e-03,  4.6134e-04,  1.1203e-02,  1.1209e-02,\n",
      "         3.6931e-03,  1.6482e-02,  8.3419e-03,  4.8904e-03,  1.5583e-02,\n",
      "         8.2115e-03,  7.9537e-03, -2.6445e-04,  4.3672e-03,  1.4464e-02,\n",
      "         1.8766e-02, -1.2710e-02,  1.9592e-02,  5.8132e-04,  7.1894e-03,\n",
      "        -6.0858e-03,  4.4346e-06,  3.5403e-03,  7.2427e-03,  4.2561e-03,\n",
      "         2.4301e-02,  1.5068e-04, -3.9530e-04, -1.3063e-04, -4.6473e-03,\n",
      "         1.3286e-03,  1.7594e-03,  2.9521e-03,  3.8993e-03,  9.1528e-03,\n",
      "         6.8479e-03,  2.5879e-02,  2.2067e-02,  7.8168e-03,  1.1650e-02,\n",
      "         4.5322e-02,  3.1838e-02,  7.9907e-03,  9.9972e-03,  1.3257e-02,\n",
      "         7.9823e-03,  2.8183e-02,  2.2041e-02,  3.8396e-03,  7.5894e-03,\n",
      "         1.8422e-02,  2.8145e-02,  1.6942e-02,  3.4547e-03,  8.1447e-03,\n",
      "         8.3253e-03,  1.3754e-02,  1.2256e-02, -1.5466e-04,  1.4245e-03,\n",
      "        -6.2712e-05, -9.9825e-03, -1.5440e-03, -2.7658e-02, -3.3798e-02,\n",
      "        -2.0405e-02, -1.0022e-02,  1.3678e-02,  6.5322e-03,  2.7218e-02,\n",
      "         1.7883e-03, -1.9160e-02,  1.2134e-02,  4.2010e-02,  8.3189e-03,\n",
      "         1.9739e-02,  2.4193e-02,  6.4606e-03,  2.3012e-02,  6.0133e-03,\n",
      "         3.4682e-03,  8.1471e-03,  1.4444e-02,  5.6068e-03, -7.3985e-04,\n",
      "         6.6370e-03,  4.7052e-03,  4.0453e-03,  1.1879e-03, -3.9533e-03,\n",
      "         1.3864e-02,  1.0048e-02,  1.2180e-02, -4.5046e-03,  4.5214e-03,\n",
      "        -7.0281e-04,  9.6569e-03,  1.5235e-02,  8.3805e-03,  1.1602e-02,\n",
      "         9.4707e-03,  9.5608e-03,  4.6018e-03,  2.3625e-02,  7.7809e-03,\n",
      "        -1.6773e-02,  6.0165e-03,  8.3538e-03,  1.6520e-02,  1.2132e-02,\n",
      "         3.4287e-02,  4.1685e-04, -5.2825e-03, -1.0561e-04, -1.9266e-02,\n",
      "        -8.4020e-03,  6.9595e-03, -5.6740e-04,  8.2052e-03, -1.5921e-02,\n",
      "         9.3193e-03,  2.3146e-01,  3.1973e-01,  1.0161e-01,  2.7361e-01,\n",
      "         2.5631e-01,  2.5385e-01,  3.0070e-01,  4.6327e-01,  5.6751e-02,\n",
      "         2.5466e-01,  1.0082e-01,  5.3161e-02, -9.9234e-05, -7.0580e-02,\n",
      "        -1.2182e-04, -3.5323e-01, -3.7445e-01, -3.8690e-01, -1.5388e-01,\n",
      "        -6.0733e-02,  1.4895e-01, -2.5248e-01, -1.1904e-02, -5.6941e-02,\n",
      "        -1.3335e-02])}))\n"
     ]
    }
   ],
   "source": [
    "print(data.guidelines)\n",
    "print(\"#\" * 10)\n",
    "print(data.system_prompt)\n",
    "print(\"#\" * 10)\n",
    "print(data.demonstrations)\n",
    "print(\"#\" * 10)\n",
    "print(data.rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bcf41f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([674])\n",
      "(0, 666)\n",
      "666\n",
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "# Here's an example on how to interpret attribution scores and spans.\n",
    "\n",
    "# The number of attributed tokens is the shape of \"attributions\". This is the tensor\n",
    "# we'll need to slice to pick the correct token spans and contributions.\n",
    "# It contains attribution scores for all the input tokens, including the chat template special ones.\n",
    "# Note: if you are checking the attributions of \"translation\", then the last positions will also\n",
    "# contain the scores of the initial output tokens that come before the translation, for example\n",
    "# <de> **GENDERED**\n",
    "iid = 28\n",
    "print(data.rows[iid].translation.metadata[\"attributions\"].shape)\n",
    "\n",
    "# Full_prompt refers to the entire input context. It's gonna be slightly shorter than the scores above\n",
    "print(data.rows[iid].full_prompt.span)\n",
    "print(len(data.rows[iid].full_prompt.tokens))\n",
    "\n",
    "# The translation label span refers to the output tokens, i.e., it's the span in the generation that\n",
    "# contains all the tokens referring to the translation label.\n",
    "print(data.rows[iid].translation_label.span)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "91654563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 392/392 [00:01<00:00, 233.00it/s]\n"
     ]
    }
   ],
   "source": [
    "def slice_array(x, l_idx, r_idx):\n",
    "    return x[l_idx:r_idx]\n",
    "\n",
    "def find_max(tensors: list):\n",
    "    \"\"\"Find the maximum value in a list of tensors.\"\"\"\n",
    "    max_value = float(\"-inf\")\n",
    "    for tensor in tensors:\n",
    "        max_value = max(max_value, tensor.max().item())\n",
    "    return max_value\n",
    "\n",
    "def find_mean_std(tensors: list):\n",
    "    t = torch.cat(tensors)\n",
    "    return t.mean(), t.std()\n",
    "\n",
    "def find_relevant_tokens(scores: torch.tensor, tokens: list[str], tr: float):\n",
    "    relevant_tokens = list()\n",
    "    for i, (s, t) in enumerate(zip(scores, tokens)):\n",
    "        if s > tr:\n",
    "            relevant_tokens.append((i, t))\n",
    "    return relevant_tokens\n",
    "\n",
    "# all_stats = list()\n",
    "# all_tensors = list()\n",
    "# row_stats = dict()\n",
    "full_json = dict()\n",
    "\n",
    "for row_attr in tqdm(data.rows):\n",
    "    scores = getattr(row_attr, ATTR_TARGET).metadata[\"attributions\"]\n",
    "    \n",
    "    row_dict = dict()\n",
    "    tokens_cat = list()\n",
    "    part_cat = list()\n",
    "    scores_cat = list()\n",
    "\n",
    "    row_dict[\"system_prompt\"] = slice_array(scores, *data.system_prompt.span)\n",
    "    tokens_cat.extend(data.system_prompt.tokens)\n",
    "    part_cat.extend([\"SYS\"] * len(data.system_prompt.tokens))\n",
    "    scores_cat.append(row_dict[\"system_prompt\"])\n",
    "\n",
    "    row_dict[\"preamble\"] = slice_array(scores, *data.preamble.span)\n",
    "    tokens_cat.extend(data.preamble.tokens)\n",
    "    part_cat.extend([\"PRE\"] * len(data.preamble.tokens))\n",
    "    scores_cat.append(row_dict[\"preamble\"])\n",
    "\n",
    "    row_dict[\"guidelines\"] = slice_array(scores, *data.guidelines.span)\n",
    "    tokens_cat.extend(data.guidelines.tokens)\n",
    "    part_cat.extend([\"GUI\"] * len(data.guidelines.tokens))\n",
    "    scores_cat.append(row_dict[\"guidelines\"])\n",
    "    \n",
    "    for i, (u, a) in enumerate(data.demonstrations):\n",
    "        scores_u = slice_array(scores, *u.span)\n",
    "        tokens_cat.extend(u.tokens)\n",
    "        scores_cat.append(scores_u)\n",
    "        part_cat.extend([f\"USR_{i}\"] * len(u.tokens))\n",
    "        \n",
    "        scores_a = slice_array(scores, *a.span)\n",
    "        tokens_cat.extend(a.tokens)\n",
    "        scores_cat.append(scores_a)\n",
    "        part_cat.extend([f\"ASS_{i}\"] * len(a.tokens))\n",
    "\n",
    "        row_dict[f\"shot_{i}_u\"] = scores_u\n",
    "        row_dict[f\"shot_{i}_a\"] = scores_a\n",
    "        # row_dict[f\"full_shot_{i}\"] = torch.cat((scores_u, scores_a))\n",
    "    \n",
    "    if not EXCLUDE_SRC_SCORES:\n",
    "        row_dict[\"source\"] = slice_array(scores, *row_attr.source.span)\n",
    "        tokens_cat.extend(row_attr.source.tokens)\n",
    "        scores_cat.append(row_dict[\"source\"])\n",
    "        part_cat.extend([\"SRC\"] * len(row_attr.source.tokens))\n",
    "\n",
    "    if ADD_TRANSLATION_LABEL:\n",
    "        tl_span = (\n",
    "            row_attr.full_prompt.span[1] + row_attr.translation_label.span[0],\n",
    "            row_attr.full_prompt.span[1] + row_attr.translation_label.span[1]\n",
    "        )\n",
    "        row_dict[\"translation_label\"] = slice_array(scores, *tl_span)\n",
    "        tokens_cat.extend(row_attr.translation_label.tokens)\n",
    "        scores_cat.append(row_dict[\"translation_label\"])\n",
    "        part_cat.extend([\"TL\"] * len(row_attr.translation_label.tokens))\n",
    "        \n",
    "    # row_dict at this point contains all the parts of the context\n",
    "    # that are relevant for the max / normalization computation\n",
    "    \n",
    "    # 1. turn everything into abs values and compute the max, mean, and std\n",
    "    row_dict = {k: v.abs() for k, v in row_dict.items()}\n",
    "    max_value = find_max([v for k, v in row_dict.items() if \"full_shot\" not in k])\n",
    "    mean_value, std_value = find_mean_std([v for k, v in row_dict.items() if \"full_shot\" not in k])\n",
    "\n",
    "    # 2. set the threshold for significance to twice the std \n",
    "    threshold = mean_value + 2 * std_value\n",
    "\n",
    "    # 3. count for each item of the dict, how many items are above the threshold\n",
    "    relevant_counts = dict()\n",
    "    relevant_tokens = dict()\n",
    "    for cp in [\"system_prompt\", \"guidelines\", \"preamble\"]:\n",
    "        rt = find_relevant_tokens(\n",
    "            row_dict[cp], getattr(data, cp).tokens, threshold\n",
    "        )\n",
    "        relevant_tokens[cp] = rt\n",
    "        relevant_counts[cp] = len(rt)\n",
    "\n",
    "    for i, shot in enumerate(data.demonstrations):\n",
    "        cp = f\"shot_{i}_u\"\n",
    "        rt = find_relevant_tokens(\n",
    "            row_dict[cp], shot[0].tokens, threshold\n",
    "        )\n",
    "        relevant_tokens[cp] = rt\n",
    "        relevant_counts[cp] = len(rt)\n",
    "        cp = f\"shot_{i}_a\"\n",
    "        rt = find_relevant_tokens(\n",
    "            row_dict[cp], shot[1].tokens, threshold\n",
    "        )\n",
    "        relevant_tokens[cp] = rt\n",
    "        relevant_counts[cp] = len(rt)\n",
    "\n",
    "    if not EXCLUDE_SRC_SCORES:\n",
    "        rt = find_relevant_tokens(\n",
    "            row_dict[\"source\"], row_attr.source.tokens, threshold\n",
    "        )\n",
    "        relevant_tokens[\"source\"] = rt\n",
    "        relevant_counts[\"source\"] = len(rt)\n",
    "\n",
    "    if ADD_TRANSLATION_LABEL:\n",
    "        rt = find_relevant_tokens(\n",
    "            row_dict[\"translation_label\"], row_attr.translation_label.tokens, threshold\n",
    "        )\n",
    "        relevant_tokens[\"translation_label\"] = rt\n",
    "        relevant_counts[\"translation_label\"] = len(rt)\n",
    "\n",
    "    # print(relevant_counts)\n",
    "    # print(relevant_tokens)\n",
    "    \n",
    "    # 4. rescale in [0,1] and compute the mean score\n",
    "    mean_dict = {\n",
    "        k: (v / max_value).mean().item() for k, v in row_dict.items()\n",
    "    }\n",
    "    # print(mean_dict)\n",
    "    # normalize everything by the max value\n",
    "    # normalized_tensors = {k: v / max_value for k, v in row_dict.items()}\n",
    "\n",
    "    # 5. get stats about the top N tokens\n",
    "    scores_cat = torch.cat(scores_cat)\n",
    "    N = 20\n",
    "    # Get the indices of the top N scores in descending order\n",
    "    top_indices = torch.argsort(scores_cat, descending=True)[:N]\n",
    "\n",
    "    # Extract the top N values, parts, and tokens based on the indices\n",
    "    top_scores = scores_cat[top_indices].tolist()\n",
    "    top_parts = [part_cat[i] for i in top_indices]\n",
    "    top_tokens = [tokens_cat[i] for i in top_indices]\n",
    "\n",
    "    full_json[row_attr.rid] = dict()\n",
    "    full_json[row_attr.rid][\"translation_label\"] = translation_df.loc[row_attr.rid][\"translation_label\"]\n",
    "    full_json[row_attr.rid][\"gold_neutrality_label\"] = translation_df.loc[row_attr.rid].get(\"gold_neutrality_label\", None)\n",
    "    full_json[row_attr.rid][\"SRC\"] = translation_df.loc[row_attr.rid][\"SRC\"]\n",
    "    full_json[row_attr.rid][\"relevant_counts\"] = relevant_counts\n",
    "    full_json[row_attr.rid][\"relevant_tokens\"] = relevant_tokens\n",
    "    full_json[row_attr.rid][\"max_value\"] = max_value\n",
    "    full_json[row_attr.rid][\"mean_value\"] = mean_value.item()\n",
    "    full_json[row_attr.rid][\"std_value\"] = std_value.item()\n",
    "    full_json[row_attr.rid][\"top_scores\"] = top_scores\n",
    "    full_json[row_attr.rid][\"top_parts\"] = top_parts\n",
    "    full_json[row_attr.rid][\"top_tokens\"] = top_tokens\n",
    "    full_json[row_attr.rid][\"mean_values\"] = mean_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4a8e849d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['system_prompt', 'preamble', 'guidelines', 'shot_0_u', 'shot_0_a', 'shot_1_u', 'shot_1_a', 'shot_2_u', 'shot_2_a', 'shot_3_u', 'shot_3_a', 'translation_label'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cb2130a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(full_json).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "acc60b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation_label</th>\n",
       "      <th>gold_neutrality_label</th>\n",
       "      <th>SRC</th>\n",
       "      <th>relevant_counts</th>\n",
       "      <th>relevant_tokens</th>\n",
       "      <th>max_value</th>\n",
       "      <th>mean_value</th>\n",
       "      <th>std_value</th>\n",
       "      <th>top_scores</th>\n",
       "      <th>top_parts</th>\n",
       "      <th>top_tokens</th>\n",
       "      <th>mean_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>**NEUTRAL**</td>\n",
       "      <td>None</td>\n",
       "      <td>May I also warn against granting migrant women...</td>\n",
       "      <td>{'system_prompt': 0, 'guidelines': 1, 'preambl...</td>\n",
       "      <td>{'system_prompt': [], 'guidelines': [(55, 'ien...</td>\n",
       "      <td>0.25248</td>\n",
       "      <td>0.008324</td>\n",
       "      <td>0.017153</td>\n",
       "      <td>[0.14895403385162354, 0.10928550362586975, 0.0...</td>\n",
       "      <td>[TL, GUI, USR_2, SYS, ASS_2, ASS_2, USR_2, USR...</td>\n",
       "      <td>[ **, ien,  legitimate,  Spanish,  veces, .], ...</td>\n",
       "      <td>{'system_prompt': 0.04472806677222252, 'preamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>**NEUTRAL**</td>\n",
       "      <td>None</td>\n",
       "      <td>This time I will give the Commissioner a chanc...</td>\n",
       "      <td>{'system_prompt': 1, 'guidelines': 2, 'preambl...</td>\n",
       "      <td>{'system_prompt': [(4, ' Spanish')], 'guidelin...</td>\n",
       "      <td>0.163181</td>\n",
       "      <td>0.014325</td>\n",
       "      <td>0.01686</td>\n",
       "      <td>[0.1631811410188675, 0.13878124952316284, 0.10...</td>\n",
       "      <td>[PRE, TL, SYS, ASS_2, TL, USR_2, USR_2, USR_2,...</td>\n",
       "      <td>[ Spanish,  **,  Spanish,  veces, **,  legitim...</td>\n",
       "      <td>{'system_prompt': 0.13752689957618713, 'preamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>**NEUTRAL**</td>\n",
       "      <td>None</td>\n",
       "      <td>I would like to address our colleague and than...</td>\n",
       "      <td>{'system_prompt': 1, 'guidelines': 1, 'preambl...</td>\n",
       "      <td>{'system_prompt': [(4, ' Spanish')], 'guidelin...</td>\n",
       "      <td>0.474094</td>\n",
       "      <td>0.014127</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>[0.10701923817396164, 0.08874690532684326, 0.0...</td>\n",
       "      <td>[PRE, GUI, SYS, ASS_2, ASS_2, GUI, ASS_1, ASS_...</td>\n",
       "      <td>[ Spanish,  coleg,  Spanish, .],  veces, as, i...</td>\n",
       "      <td>{'system_prompt': 0.025611044839024544, 'pream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>**NEUTRAL**</td>\n",
       "      <td>None</td>\n",
       "      <td>Since the Commissioner doubts whether sufficie...</td>\n",
       "      <td>{'system_prompt': 2, 'guidelines': 3, 'preambl...</td>\n",
       "      <td>{'system_prompt': [(4, ' Spanish'), (11, '.')]...</td>\n",
       "      <td>0.265968</td>\n",
       "      <td>0.022166</td>\n",
       "      <td>0.024026</td>\n",
       "      <td>[0.2659681439399719, 0.19200143218040466, 0.13...</td>\n",
       "      <td>[TL, GUI, SYS, PRE, USR_2, USR_2, USR_2, TL, U...</td>\n",
       "      <td>[**, ien,  Spanish,  Spanish,  visitors,  legi...</td>\n",
       "      <td>{'system_prompt': 0.10917984694242477, 'preamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>**NEUTRAL**</td>\n",
       "      <td>None</td>\n",
       "      <td>Many of them have lost their jobs and, with th...</td>\n",
       "      <td>{'system_prompt': 2, 'guidelines': 2, 'preambl...</td>\n",
       "      <td>{'system_prompt': [(4, ' Spanish'), (8, ' gend...</td>\n",
       "      <td>0.18237</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>0.020964</td>\n",
       "      <td>[0.18237033486366272, 0.14360371232032776, 0.1...</td>\n",
       "      <td>[TL, PRE, ASS_3, SYS, GUI, ASS_2, ASS_3, GUI, ...</td>\n",
       "      <td>[ **,  Spanish,  las,  Spanish,  personas,  ve...</td>\n",
       "      <td>{'system_prompt': 0.16190151870250702, 'preamb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   translation_label gold_neutrality_label  \\\n",
       "26       **NEUTRAL**                  None   \n",
       "35       **NEUTRAL**                  None   \n",
       "47       **NEUTRAL**                  None   \n",
       "49       **NEUTRAL**                  None   \n",
       "82       **NEUTRAL**                  None   \n",
       "\n",
       "                                                  SRC  \\\n",
       "26  May I also warn against granting migrant women...   \n",
       "35  This time I will give the Commissioner a chanc...   \n",
       "47  I would like to address our colleague and than...   \n",
       "49  Since the Commissioner doubts whether sufficie...   \n",
       "82  Many of them have lost their jobs and, with th...   \n",
       "\n",
       "                                      relevant_counts  \\\n",
       "26  {'system_prompt': 0, 'guidelines': 1, 'preambl...   \n",
       "35  {'system_prompt': 1, 'guidelines': 2, 'preambl...   \n",
       "47  {'system_prompt': 1, 'guidelines': 1, 'preambl...   \n",
       "49  {'system_prompt': 2, 'guidelines': 3, 'preambl...   \n",
       "82  {'system_prompt': 2, 'guidelines': 2, 'preambl...   \n",
       "\n",
       "                                      relevant_tokens max_value mean_value  \\\n",
       "26  {'system_prompt': [], 'guidelines': [(55, 'ien...   0.25248   0.008324   \n",
       "35  {'system_prompt': [(4, ' Spanish')], 'guidelin...  0.163181   0.014325   \n",
       "47  {'system_prompt': [(4, ' Spanish')], 'guidelin...  0.474094   0.014127   \n",
       "49  {'system_prompt': [(4, ' Spanish'), (11, '.')]...  0.265968   0.022166   \n",
       "82  {'system_prompt': [(4, ' Spanish'), (8, ' gend...   0.18237   0.018524   \n",
       "\n",
       "   std_value                                         top_scores  \\\n",
       "26  0.017153  [0.14895403385162354, 0.10928550362586975, 0.0...   \n",
       "35   0.01686  [0.1631811410188675, 0.13878124952316284, 0.10...   \n",
       "47  0.025933  [0.10701923817396164, 0.08874690532684326, 0.0...   \n",
       "49  0.024026  [0.2659681439399719, 0.19200143218040466, 0.13...   \n",
       "82  0.020964  [0.18237033486366272, 0.14360371232032776, 0.1...   \n",
       "\n",
       "                                            top_parts  \\\n",
       "26  [TL, GUI, USR_2, SYS, ASS_2, ASS_2, USR_2, USR...   \n",
       "35  [PRE, TL, SYS, ASS_2, TL, USR_2, USR_2, USR_2,...   \n",
       "47  [PRE, GUI, SYS, ASS_2, ASS_2, GUI, ASS_1, ASS_...   \n",
       "49  [TL, GUI, SYS, PRE, USR_2, USR_2, USR_2, TL, U...   \n",
       "82  [TL, PRE, ASS_3, SYS, GUI, ASS_2, ASS_3, GUI, ...   \n",
       "\n",
       "                                           top_tokens  \\\n",
       "26  [ **, ien,  legitimate,  Spanish,  veces, .], ...   \n",
       "35  [ Spanish,  **,  Spanish,  veces, **,  legitim...   \n",
       "47  [ Spanish,  coleg,  Spanish, .],  veces, as, i...   \n",
       "49  [**, ien,  Spanish,  Spanish,  visitors,  legi...   \n",
       "82  [ **,  Spanish,  las,  Spanish,  personas,  ve...   \n",
       "\n",
       "                                          mean_values  \n",
       "26  {'system_prompt': 0.04472806677222252, 'preamb...  \n",
       "35  {'system_prompt': 0.13752689957618713, 'preamb...  \n",
       "47  {'system_prompt': 0.025611044839024544, 'pream...  \n",
       "49  {'system_prompt': 0.10917984694242477, 'preamb...  \n",
       "82  {'system_prompt': 0.16190151870250702, 'preamb...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2f674f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(full_json, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
